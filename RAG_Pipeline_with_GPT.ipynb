{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oKeOJ3XfCCx6",
        "outputId": "bc4c08b8-921f-48c7-e20f-6ca99ac801e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.57.0)\n",
            "Requirement already satisfied: faiss-gpu in /usr/local/lib/python3.10/dist-packages (1.7.2)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.2.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (4.7.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.8.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.10.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.46.3)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.5.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.26.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (11.0.0)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.27.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install openai faiss-gpu sentence-transformers pandas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZWv42y5FabM",
        "outputId": "99ae2984-7271-4e8d-a42a-3d15076f60a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.57.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (4.7.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.8.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.10.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.27.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcv_gemNCKS0",
        "outputId": "3a87b58e-a4fe-4537-ecda-41f22fcf77c2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm, trange\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import faiss\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import json\n",
        "\n",
        "# Load structured financial dataset\n",
        "df = pd.read_csv(\"structured_10k.csv\")\n",
        "\n",
        "# Load pre-trained embeddings model for financial text\n",
        "model = SentenceTransformer(\"FinLang/finance-embeddings-investopedia\")\n",
        "\n",
        "# Combine headers and content into a single list for embeddings\n",
        "columns = df.columns.tolist()\n",
        "content_list = [f\"{col}: {df[col].iloc[0]}\" for col in columns]  # Use the first row's content\n",
        "\n",
        "# Generate embeddings\n",
        "embeddings = model.encode(content_list)\n",
        "embedding_array = np.array(embeddings)\n",
        "\n",
        "# Initialize FAISS index\n",
        "dimension = embedding_array.shape[1]\n",
        "faiss_index = faiss.IndexFlatL2(dimension)\n",
        "\n",
        "# Add embeddings to the FAISS index\n",
        "faiss_index.add(embedding_array)\n",
        "\n",
        "# Save column names for reverse mapping\n",
        "section_mapping = {i: col for i, col in enumerate(columns)}\n",
        "\n",
        "# Save the FAISS index and mapping for reuse\n",
        "faiss.write_index(faiss_index, \"finance_10k_index.faiss\")\n",
        "with open(\"section_mapping.json\", \"w\") as f:\n",
        "    json.dump(section_mapping, f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "BrJQc8q-pLzd",
        "outputId": "50f1aae5-767b-4935-953e-5f868e0b0ef6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (2024.9.11)\n",
            "Collecting regex\n",
            "  Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4 in /usr/local/lib/python3.10/dist-packages (3.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (3.10)\n",
            "Requirement already satisfied: urllib3<3 in /usr/local/lib/python3.10/dist-packages (2.2.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (2024.8.30)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: anyio<5 in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Collecting anyio<5\n",
            "  Downloading anyio-4.7.0-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: distro<2 in /usr/local/lib/python3.10/dist-packages (1.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (1.3.1)\n",
            "Requirement already satisfied: h11<0.15 in /usr/local/lib/python3.10/dist-packages (0.14.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (1.0.7)\n",
            "Requirement already satisfied: annotated-types in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "Requirement already satisfied: typing-extensions<5 in /usr/local/lib/python3.10/dist-packages (4.12.2)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (2.27.1)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.10/dist-packages (2.10.2)\n",
            "Collecting pydantic<3\n",
            "  Downloading pydantic-2.10.3-py3-none-any.whl.metadata (172 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.0/172.0 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jiter<1 in /usr/local/lib/python3.10/dist-packages (0.8.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.6)\n",
            "Collecting tqdm\n",
            "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting colorama\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.57.0)\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting httpx<0.28\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio<5) (1.2.2)\n",
            "Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.7/781.7 kB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading anyio-4.7.0-py3-none-any.whl (93 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic-2.10.3-py3-none-any.whl (456 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m457.0/457.0 kB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m45.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tqdm, regex, colorama, anyio, tiktoken, pydantic, httpx\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.66.6\n",
            "    Uninstalling tqdm-4.66.6:\n",
            "      Successfully uninstalled tqdm-4.66.6\n",
            "  Attempting uninstall: regex\n",
            "    Found existing installation: regex 2024.9.11\n",
            "    Uninstalling regex-2024.9.11:\n",
            "      Successfully uninstalled regex-2024.9.11\n",
            "  Attempting uninstall: anyio\n",
            "    Found existing installation: anyio 3.7.1\n",
            "    Uninstalling anyio-3.7.1:\n",
            "      Successfully uninstalled anyio-3.7.1\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.10.2\n",
            "    Uninstalling pydantic-2.10.2:\n",
            "      Successfully uninstalled pydantic-2.10.2\n",
            "  Attempting uninstall: httpx\n",
            "    Found existing installation: httpx 0.28.0\n",
            "    Uninstalling httpx-0.28.0:\n",
            "      Successfully uninstalled httpx-0.28.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jupyter-server 1.24.0 requires anyio<4,>=3.1.0, but you have anyio 4.7.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed anyio-4.7.0 colorama-0.4.6 httpx-0.27.2 pydantic-2.10.3 regex-2024.11.6 tiktoken-0.8.0 tqdm-4.67.1\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "7d237a0282eb4f8a9c21597b34011d49",
              "pip_warning": {
                "packages": [
                  "tqdm"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install --upgrade --upgrade-strategy eager \"regex\" \"charset-normalizer<4\" \"idna\" \"urllib3<3\" \"certifi\" \"requests\" \"anyio<5\" \"distro<2\" \"sniffio\" \"h11<0.15\" \"httpcore==1.*\"  \"annotated-types\" \"typing-extensions<5\" \"pydantic-core==2.27.1\" \"pydantic<3\" \"jiter<1\" \"tqdm\" \"colorama\" \"openai\" \"tiktoken\" \"httpx<0.28\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-TvBm9fepGx-"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "# Initialize the client\n",
        "client = OpenAI(\n",
        "    api_key=\"\"\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "_G5xe5fRDD15"
      },
      "outputs": [],
      "source": [
        "def rag_pipeline_with_gpt(query, faiss_index, section_mapping, df, model, top_k=3):\n",
        "    \"\"\"\n",
        "    Retrieval-Augmented Generation pipeline using FAISS and OpenAI GPT.\n",
        "\n",
        "    :param query: The user query.\n",
        "    :param faiss_index: The FAISS index for document retrieval.\n",
        "    :param section_mapping: Mapping of FAISS indices to section names.\n",
        "    :param df: DataFrame containing structured financial data.\n",
        "    :param model: Sentence transformer model for embedding the query.\n",
        "    :param top_k: Number of top results to retrieve.\n",
        "    :return: GPT-generated response.\n",
        "    \"\"\"\n",
        "    # Step 1: Encode the query\n",
        "    query_embedding = model.encode([query])\n",
        "\n",
        "    # Step 2: Search FAISS index\n",
        "    distances, indices = faiss_index.search(np.array(query_embedding), k=top_k)\n",
        "\n",
        "    # Step 3: Retrieve relevant sections\n",
        "    context = \"\"\n",
        "    for idx in indices[0]:\n",
        "        section_name = section_mapping[str(idx)]\n",
        "        section_content = df[section_name].iloc[0]\n",
        "        context += f\"Section: {section_name}\\n{section_content}\\n\\n\"\n",
        "\n",
        "    # Step 4: Generate a response using GPT\n",
        "    prompt = f\"\"\"Below is a user query paired with retrieved context from financial documents.\n",
        "    Write a detailed, fact-based response that answers the query.\n",
        "\n",
        "    ### Query:\n",
        "    {query}\n",
        "\n",
        "    ### Context:\n",
        "    {context}\n",
        "\n",
        "    ### Response:\"\"\"\n",
        "    response = client.chat.completions.create(\n",
        "    model=\"gpt-4o\",  # Model ID (ensure you have access)\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a financial analyst assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": prompt},\n",
        "    ],)\n",
        "\n",
        "    return response\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "JgNW55quDGlC"
      },
      "outputs": [],
      "source": [
        "import faiss\n",
        "import json\n",
        "import pandas as pd\n",
        "# Load FAISS index and section mapping\n",
        "faiss_index = faiss.read_index(\"finance_10k_index.faiss\")\n",
        "with open(\"section_mapping.json\", \"r\") as f:\n",
        "    section_mapping = json.load(f)\n",
        "# Load structured financial dataset\n",
        "df = pd.read_csv(\"structured_10k.csv\")\n",
        "\n",
        "# Query example\n",
        "query = \"What are the risk factors mentioned in the report?\"\n",
        "\n",
        "# Generate response\n",
        "response = rag_pipeline_with_gpt(query, faiss_index, section_mapping, df, model)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VI3mabv-qsGI",
        "outputId": "d01b9bf4-58ed-459f-c6ab-3e0e27b9bb6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated Response:\n",
            "ChatCompletion(id='chatcmpl-AbUt4JpVm47uJ51DyPJRCfgvZMtCb', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"The report outlines several risk factors that could materially and adversely impact the company's business, reputation, operational results, financial condition, and stock price. Here are the key risk factors mentioned in the context:\\n\\n1. **Interest Rate Risk**: The company is exposed to fluctuations in U.S. interest rates, which can affect both its investment portfolio and its term debt. An increase in interest rates could lead to a decrease in the fair value of the company's investment portfolio, as well as an uptick in interest expenses related to its term debt. The report notes a hypothetical scenario where a 100 basis point rise in interest rates could cause a decline in the portfolio's fair value by $2,755 million in 2024 and $3,089 million in 2023, and increase the annual interest expense by $139 million in 2024 and $194 million in 2023.\\n\\n2. **Foreign Exchange Rate Risk**: The company is also susceptible to risks from changes in foreign exchange rates, primarily because it receives currencies other than the U.S. dollar. A strengthening U.S. dollar could negatively impact the company's net sales and gross margins when expressed in U.S. dollars, besides affecting the fair values of some of its assets and liabilities. Measures like using derivative instruments, hedging, and adjusting local currency pricing are in place to manage this risk, though the company might choose not to hedge certain exposures for various reasons.\\n\\nMoreover, the company utilizes a Value-at-Risk (VAR) model to understand the potential impact of exchange rate fluctuations on its foreign currency derivative positions. As per this model, as of September 28, 2024, and September 30, 2023, the estimated maximum one-day loss, with 95% confidence, in the fair value due to such rate fluctuations was $538 million and $669 million, respectively.\\n\\nThese risk factors underscore the inherent volatility and uncertainty the company faces in its financial operations, which investors should consider when evaluating the company's past performance and projecting future outcomes.\", refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1733500230, model='gpt-4o-2024-08-06', object='chat.completion', service_tier=None, system_fingerprint='fp_9d50cd990b', usage=CompletionUsage(completion_tokens=405, prompt_tokens=838, total_tokens=1243, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n"
          ]
        }
      ],
      "source": [
        "print(\"Generated Response:\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4W4ZtDUhJ23W",
        "outputId": "b7043e0f-ad43-4be3-87f8-111e364ae873"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Name: openai\n",
            "Version: 1.57.0\n",
            "Summary: The official Python library for the openai API\n",
            "Home-page: https://github.com/openai/openai-python\n",
            "Author: \n",
            "Author-email: OpenAI <support@openai.com>\n",
            "License: Apache-2.0\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: anyio, distro, httpx, jiter, pydantic, sniffio, tqdm, typing-extensions\n",
            "Required-by: \n"
          ]
        }
      ],
      "source": [
        "!pip show openai\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I1zikHJ2J3ZZ",
        "outputId": "d5c026e9-eaf1-4374-82d4-70177ecb5b61"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated Content: The report outlines several risk factors that could materially and adversely impact the company's business, reputation, operational results, financial condition, and stock price. Here are the key risk factors mentioned in the context:\n",
            "\n",
            "1. **Interest Rate Risk**: The company is exposed to fluctuations in U.S. interest rates, which can affect both its investment portfolio and its term debt. An increase in interest rates could lead to a decrease in the fair value of the company's investment portfolio, as well as an uptick in interest expenses related to its term debt. The report notes a hypothetical scenario where a 100 basis point rise in interest rates could cause a decline in the portfolio's fair value by $2,755 million in 2024 and $3,089 million in 2023, and increase the annual interest expense by $139 million in 2024 and $194 million in 2023.\n",
            "\n",
            "2. **Foreign Exchange Rate Risk**: The company is also susceptible to risks from changes in foreign exchange rates, primarily because it receives currencies other than the U.S. dollar. A strengthening U.S. dollar could negatively impact the company's net sales and gross margins when expressed in U.S. dollars, besides affecting the fair values of some of its assets and liabilities. Measures like using derivative instruments, hedging, and adjusting local currency pricing are in place to manage this risk, though the company might choose not to hedge certain exposures for various reasons.\n",
            "\n",
            "Moreover, the company utilizes a Value-at-Risk (VAR) model to understand the potential impact of exchange rate fluctuations on its foreign currency derivative positions. As per this model, as of September 28, 2024, and September 30, 2023, the estimated maximum one-day loss, with 95% confidence, in the fair value due to such rate fluctuations was $538 million and $669 million, respectively.\n",
            "\n",
            "These risk factors underscore the inherent volatility and uncertainty the company faces in its financial operations, which investors should consider when evaluating the company's past performance and projecting future outcomes.\n"
          ]
        }
      ],
      "source": [
        "# Extract and print just the content\n",
        "generated_content = response.choices[0].message.content\n",
        "print(\"Generated Content:\", generated_content)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQn0_hygtMmp",
        "outputId": "eca94210-53a5-4466-ae37-2c4678de311f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated Content: Based on the provided context from the company's financial documents, the company manages its business primarily on a geographic basis with reportable segments including the Americas, Europe, Greater China, Japan, and the Rest of Asia Pacific. While the context does not specify which of these regions contributes the most to international sales, it highlights the segmentation of markets that typically cover international operations.\n",
            "\n",
            "For a more definitive answer, one would need specific financial data regarding sales distribution among these segments. However, historically for many large multinational companies in the tech industry, the Americas (particularly North America) often make significant contributions to overall sales, but in terms of international regions, Greater China and Europe are usually substantial contributors as well.\n",
            "\n",
            "To gain a precise understanding of which region contributes most to international sales for the company, it would be advisable to refer to the company's detailed financial statements or investor reports that break down revenue by region within the fiscal year in question. These documents often provide clearer insights into the performance and contributions of each geographic segment.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Query example\n",
        "query = \"What region contributes most to international sales?\"\n",
        "\n",
        "# Generate response\n",
        "response = rag_pipeline_with_gpt(query, faiss_index, section_mapping, df, model)\n",
        "generated_content = response.choices[0].message.content\n",
        "print(\"Generated Content:\", generated_content)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZjQdnwPtPCb",
        "outputId": "a65bb7cb-187e-4421-b601-ea6ef3ec5f55"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Object `Sales` not found.\n",
            "Generated Content: Based on the information provided in the context, Apple Inc. manages its business primarily on a geographic basis and has identified several reportable segments: the Americas, Europe, Greater China, Japan, and Rest of Asia Pacific. Each of these segments provides similar hardware and software products and services, and they are managed separately to align with the location of the company's customers and distribution partners as well as the unique market dynamics of each region. \n",
            "\n",
            "To determine which region contributes most to Apple's international sales, we need to look at the sales data across these segments. However, the context provided does not include specific sales figures for each geographic segment. Typically, Apple's Greater China region, comprising China mainland, Hong Kong, and Taiwan, has been one of the significant contributors to its international sales owing to its large consumer base and growing middle class. Europe also plays a vital role as it includes European countries, India, the Middle East, and Africa, providing a broad market base.\n",
            "\n",
            "In the absence of specific sales numbers in the context, it is challenging to definitively identify the leading contributor to international sales. Typically, Apple’s past financial reports have highlighted Greater China and Europe as significant contributors to international sales, but without specific data, this remains a generalization that might not fully encapsulate the current fiscal year's performance.\n",
            "\n",
            "For the most accurate analysis, one would typically refer to Apple's annual 10-K reports or quarterly earnings calls where the company breaks down the sales and performance of each region. These documents would provide concrete data on the contribution of each region to international sales.\n"
          ]
        }
      ],
      "source": [
        "query =\"Where is outsourcing located currently? iPhone Net Sales?\"\n",
        "# Generate response\n",
        "response = rag_pipeline_with_gpt(query, faiss_index, section_mapping, df, model)\n",
        "generated_content = response.choices[0].message.content\n",
        "print(\"Generated Content:\", generated_content)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QEZeQF76tgqz"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
